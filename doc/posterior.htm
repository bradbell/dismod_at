<html>
<script type='text/x-mathjax-config'>
MathJax.Hub.Config({
  tex2jax: {
    inlineMath:  [ ['@(@','@)@'] ] ,
    displayMath: [ ['@[@','@]@'] ]
  }
});
</script>
<script type='text/javascript' src=
'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=default'
>
</script>
<head>
<title>Simulating Posterior Distribution for Model Variables</title>
<meta http-equiv='Content-Type' content='text/html' charset='utf-8'>
<meta name="description" id="description" content="Simulating Posterior Distribution for Model Variables">
<meta name="keywords" id="keywords" content=" ">
<style type='text/css'>
body { color : black }
body { background-color : white }
A:link { color : blue }
A:visited { color : purple }
A:active { color : purple }
</style>
<script type='text/javascript' language='JavaScript' src='_posterior_htm.js'>
</script>
</head>
<body>
<table><tr>
<td>
<a href="https://bradbell.github.io/dismod_at" target="_top"><img border="0" src="_image.gif"></a>
</td>
<td><a href="bilinear.htm" target="_top">Prev</a>
</td><td><a href="prev_dep.htm" target="_top">Next</a>
</td><td>
<select onchange='choose_across0(this)'>
<option>Index-&gt;</option>
<option>contents</option>
<option>reference</option>
<option>index</option>
<option>search</option>
<option>external</option>
</select>
</td>
<td>
<select onchange='choose_up0(this)'>
<option>Up-&gt;</option>
<option>dismod_at</option>
<option>model</option>
<option>posterior</option>
</select>
</td>
<td>
<script type='text/javascript' language='JavaScript' src='_childtable_dismod_at_htm.js'></script>
</td>
<td>
<script type='text/javascript' language='JavaScript' src='_childtable_model_htm.js'></script>
</td>
<td>posterior</td>
</tr></table><br>
@(@\newcommand{\B}[1]{ {\bf #1} }
\newcommand{\R}[1]{ {\rm #1} }
\newcommand{\W}[1]{ \; #1 \; }@)@



<center><b><big><big>Simulating Posterior Distribution for Model Variables</big></big></b></center>

<br><a href="posterior.htm#Purpose" target="_top">Purpose</a>
<br><a href="posterior.htm#Lemma 1" target="_top">Lemma&nbsp;1</a>
<br>&#160;&#160;&#160;&#160;&#160;<a href="posterior.htm#Lemma 1.Proof" target="_top">Proof</a>
<br>&#160;&#160;&#160;&#160;&#160;<a href="posterior.htm#Lemma 1.Remark" target="_top">Remark</a>
<br><a href="posterior.htm#Lemma 2" target="_top">Lemma&nbsp;2</a>
<br>&#160;&#160;&#160;&#160;&#160;<a href="posterior.htm#Lemma 2.Remark" target="_top">Remark</a>
<br>&#160;&#160;&#160;&#160;&#160;<a href="posterior.htm#Lemma 2.Proof" target="_top">Proof</a>
<br><a href="posterior.htm#Simulation" target="_top">Simulation</a>
<br><br>
<b><big><a name="Purpose" id="Purpose">Purpose</a></big></b>
<br>
The <code><font color="blue">sample</font></code> command with method equal to
<a href="sample_command.htm#simulate" target="_top"><span style='white-space: nowrap'>simulate</span></a>

fits simulated random measurements <a href="data_sim_table.htm" target="_top"><span style='white-space: nowrap'>data_sim_table</span></a>

and simulated random priors <a href="prior_sim_table.htm" target="_top"><span style='white-space: nowrap'>prior_sim_table</span></a>
.
The Lemmas on this page prove that, in the linear Gaussian case,
this gives the proper statistics for the posterior distribution
of the maximum likelihood estimate.
Note that the dismod_at case is not linear nor are all the
statistics Gaussian.


<br>
<br>
<b><big><a name="Lemma 1" id="Lemma 1">Lemma 1</a></big></b>
<br>
Suppose we are given
a matrix <small>@(@
A \in \B{R}^{m \times n}
@)@</small>,
a positive definite matrix, <small>@(@
V \in \B{R}^{m \times m}
@)@</small> and
a <small>@(@
y \in \B{R}^{m \times 1}
@)@</small>.
Further suppose that there is an
unknown vector <small>@(@
\bar{\theta} \in \B{R}^{n \times 1}
@)@</small> such that
<small>@(@
y \sim \B{N} ( A \bar{\theta} , V )
@)@</small>.
The maximum likelihood estimator <small>@(@
\hat{\theta}
@)@</small>
for <small>@(@
\bar{\theta}
@)@</small> given <small>@(@
y
@)@</small> has mean
<small>@(@
\B{E} [ \hat{\theta} ] = \bar{\theta}
@)@</small> and variance
<small>@[@

\B{E} [ ( \hat{\theta} - \bar{\theta} ) ( \hat{\theta} - \bar{\theta} )^\R{T} ]
=
( A^\R{T} V^{-1} A )^{-1}
=
- \; \left(
   \frac{ \partial^2 } { \partial \theta^2 } \log \B{p} ( y | \theta )
\right)^{-1}

@]@</small>


<br>
<br>
<big><a name="Lemma 1.Proof" id="Lemma 1.Proof">Proof</a></big>
<br>
The probability density for <small>@(@
y
@)@</small> given <small>@(@
\theta
@)@</small> is
<small>@[@

\B{p} ( y | \theta )
=
\det ( 2 \pi V )^{-1/2} \exp \left[
   - \frac{1}{2} ( y - A \theta )^\R{T} V^{-1} ( y - A \theta  )
\right]

@]@</small>
Dropping the determinant term,
because it does not depend on <small>@(@
\theta
@)@</small>,
and taking the negative log we get the objective
<small>@[@

f ( \theta ) = \frac{1}{2} ( y - A \theta )^\R{T} V^{-1} ( y - A \theta  )

@]@</small>
and the equivalent problem to minimize <small>@(@
f ( \theta )
@)@</small> with
respect to <small>@(@
\theta \in \B{R}^{n \times 1}
@)@</small>.
The derivative <small>@(@
f^{(1)} ( \theta ) \in \B{R}^{1 \times n}
@)@</small>
is given by
<small>@[@

   f^{(1)} ( \theta ) = - ( y - A \theta )^\R{T} V^{-1} A

@]@</small>
It follows that
<small>@[@

- \frac{ \partial^2 } { \partial \theta^2 } \log \B{p} ( y | \theta )
=
f^{(2)} ( \theta )
=
A^\R{T} V^{-1} A

@]@</small>
This completes the proof of the equation for the second partial
of <small>@(@
\B{p} ( y | \theta )
@)@</small> in the statement of the lemma.
<code><span style='white-space: nowrap'><br>
&nbsp;&nbsp;&nbsp;</span></code>
The maximum likelihood estimate <small>@(@
\hat{\theta}
@)@</small> satisfies the
equation <small>@(@
f^{(1)} ( \hat{\theta} ) = 0
@)@</small>; i.e.,
<small>@[@

\begin{array}{rcl}
0 & = & - ( y - A \hat{\theta} )^\R{T} V^{-1} A
\\
A^\R{T} V^{-1} y  & = & A^\R{T} V^{-1} A \hat{\theta}
\\
\hat{\theta} & =  & ( A^\R{T} V^{-1} A )^{-1} A^\R{T} V^{-1} y
\end{array}

@]@</small>
Defining <small>@(@
e = y - A \bar{\theta}
@)@</small>, we have
<small>@(@
\B{E} [ e ] = 0
@)@</small> and
<small>@[@

\begin{array}{rcl}
\hat{\theta} & =  &
( A^\R{T} V^{-1} A )^{-1} A^\R{T} V^{-1} ( A \bar{\theta} + e )
\\
\hat{\theta}
& =  &
\bar{\theta} + ( A^\R{T} V^{-1} A )^{-1} A^\R{T} V^{-1} e
\end{array}

@]@</small>
This expresses the estimate <small>@(@
\hat{\theta}
@)@</small> as a deterministic
function of the noise <small>@(@
e
@)@</small>.
It follows from the last equation for <small>@(@
\hat{\theta}
@)@</small> above,
and the fact that <small>@(@
\B{E} [ e ] = 0
@)@</small>,
that <small>@(@
\B{E} [ \hat{\theta} ] = \bar{\theta}
@)@</small>.
This completes the proof of the equation for the expected value
of <small>@(@
\hat{\theta}
@)@</small> in the statement of the lemma.
<code><span style='white-space: nowrap'><br>
&nbsp;&nbsp;&nbsp;</span></code>
It also follows, from the equation for <small>@(@
\hat{\theta}
@)@</small> above, that
<small>@[@

\begin{array}{rcl}
( \hat{\theta} - \bar{\theta} ) ( \hat{\theta} - \bar{\theta} )^\R{T}
& = &
( A^\R{T} V^{-1} A )^{-1} A^\R{T}
V^{-1} e e^\R{T} V^{-1}
A ( A^\R{T} V^{-1} A )^{-1}
\\
\B{E} [ ( \hat{\theta} - \bar{\theta} ) ( \hat{\theta} - \bar{\theta} )^\R{T} ]
& = &
( A^\R{T} V^{-1} A )^{-1}
\end{array}

@]@</small>
This completes the proof of the equation for the covariance of
<small>@(@
\hat{\theta} - \bar{\theta}
@)@</small> in the statement of the lemma.

<br>
<br>
<big><a name="Lemma 1.Remark" id="Lemma 1.Remark">Remark</a></big>
<br>
For the case in
<a href="posterior.htm#Lemma 1" target="_top"><span style='white-space: nowrap'>Lemma&nbsp;1</span></a>
, the second partial of
<small>@(@
\log \B{p} ( y | \theta )
@)@</small> with respect to <small>@(@
\theta
@)@</small>
does not depend on <small>@(@
\theta
@)@</small> and <small>@(@
A V^{-1} A
@)@</small> is the
information matrix.

<br>
<br>
<b><big><a name="Lemma 2" id="Lemma 2">Lemma 2</a></big></b>
<br>
Suppose that in addition to all the information in
<a href="posterior.htm#Lemma 1" target="_top"><span style='white-space: nowrap'>Lemma&nbsp;1</span></a>
 we have
a matrix <small>@(@
B \in \B{R}^{p \times n}
@)@</small>,
a positive definite matrix, <small>@(@
P \in \B{R}^{p \times p}
@)@</small>,
and <small>@(@
z \in \B{R}^{p \times 1}
@)@</small> where we have
independent prior information
<small>@(@
B \theta \sim \B{N}( z , P )
@)@</small>.
Further suppose <small>@(@
B
@)@</small> has rank <small>@(@
n
@)@</small>.
For this case we define <small>@(@
\hat{\theta}
@)@</small> as the maximizer of
<small>@(@
\B{p}( y | \theta ) \B{p}( \theta )
@)@</small>.
It follows that
<small>@[@

\B{E} [
   ( \hat{\theta} - \B{E}[ \hat{\theta} ] )
   ( \hat{\theta} - \B{E}[ \hat{\theta} ] )^\R{T}
\prec
( A^\R{T} V^{-1} A + B^\R{T} P^{-1} B )^{-1}
=
- \; \left(
   \frac{ \partial^2 } { \partial \theta^2 }
   \log [ \B{p} ( y | \theta ) \B{p} ( \theta ) ]
\right)^{-1}

@]@</small>
where <small>@(@
\prec
@)@</small> means less than in a positive definite matrix sense.

<br>
<br>
<big><a name="Lemma 2.Remark" id="Lemma 2.Remark">Remark</a></big>
<br>
The posterior distribution for the maximum likelihood estimate,
when including a prior,
cannot be sampled by fitting simulated data alone.
To see this, consider the case where column one of the matrix <small>@(@
A
@)@</small>
is zero.
In this case, that data <small>@(@
y
@)@</small> does not depend on <small>@(@
\theta_1
@)@</small> and
<small>@(@
\hat{\theta}_1 = \bar{\theta}_1
@)@</small>
no matter what the value of <small>@(@
y
@)@</small>.
On the other hand,
the posterior distribution for <small>@(@
\theta_1
@)@</small>, for this case,
is the same as its prior distribution and has uncertainty.


<br>
<br>
<big><a name="Lemma 2.Proof" id="Lemma 2.Proof">Proof</a></big>

<br>
<small>@[@

\B{p} ( y | \theta ) \B{p} ( \theta )
=
\det ( 2 \pi V )^{-1/2} \det ( 2 \pi P )^{-1/2} \exp \left[
- \frac{1}{2}
   ( y - A \theta )^\R{T} V^{-1} ( y - A \theta  )
- \frac{1}{2}
   ( B \theta - z )^\R{T} P^{-1} ( B \theta - z )
\right]

@]@</small>The derivative of the corresponding negative log likelihood is
<small>@[@

- ( y - A \theta )^\R{T} V^{-1} A  + ( B \theta - z )^\R{T} P^{-1} B

@]@</small>
From this point,
the proof of the equation for the second partial is very similar to
Lemma 1 and left to the reader.
<code><span style='white-space: nowrap'><br>
&nbsp;&nbsp;&nbsp;</span></code>
Setting the derivative to zero, we get the corresponding maximum likelihood
estimate <small>@(@
\hat{\theta}
@)@</small> satisfies
<small>@[@

\begin{array}{rcl}
( y - A \hat{\theta} )^\R{T} V^{-1} A
& = &
( B \hat{\theta} - z )^\R{T} P^{-1} B
\\
y^\R{T} V^{-1} A + z^\R{T} P^{-1} B
& = &
\hat{\theta}^\R{T} A^\R{T} V^{-1} A + \hat{\theta}^\R{T} B^\R{T} P^{-1} B
\\
\hat{\theta}
& = &
( A^\R{T} V^{-1} A + B^\R{T} P^{-1} B )^{-1}
( A^\R{T} V^{-1} y + B^\R{T} P^{-1} z )
\\
\hat{\theta}
& = &
( A^\R{T} V^{-1} A + B^\R{T} P^{-1} B )^{-1}
( A^\R{T} V^{-1} A \bar{\theta} + B^\R{T} P^{-1} z )
+
( A^\R{T} V^{-1} A + B^\R{T} P^{-1} B )^{-1} A^\R{T} V^{-1} e
\end{array}

@]@</small>
The first term is deterministic and the second term is mean zero.
It follows that
<small>@[@

\begin{array}{rcl}
\B{E} [ \hat{\theta} ]
& = &
( A^\R{T} V^{-1} A + B^\R{T} P^{-1} B )^{-1}
( A^\R{T} V^{-1} A \bar{\theta} + B^\R{T} P^{-1} z )
\\
\B{E} [
   ( \hat{\theta} - \B{E} [ \hat{\theta} ] )
   ( \hat{\theta} - \B{E} [ \hat{\theta} ] )^\R{T}
]
& = &
( A^\R{T} V^{-1} A + B^\R{T} P^{-1} B )^{-1}
A^\R{T} V^{-1} A
( A^\R{T} V^{-1} A + B^\R{T} P^{-1} B )^{-1}
\end{array}

@]@</small>
Since the matrix <small>@(@
B^\R{T} P^{-1} B
@)@</small> is positive definite, we have
<small>@[@

A^\R{T} V^{-1} A  \prec A^\R{T} V^{-1} A + B^\R{T} P^{-1} B

@]@</small>
Replacing
<small>@(@
A^\R{T} V^{-1} A
@)@</small> by
<small>@(@
A^\R{T} V^{-1} A + B^\R{T} P^{-1} B
@)@</small>
in the center of the previous expression
for the variance of <small>@(@
\hat{\theta}
@)@</small> we obtain
<small>@[@

\B{E} [
   ( \hat{\theta} - \B{E} [ \hat{\theta} ] )
   ( \hat{\theta} - \B{E} [ \hat{\theta} ] )^\R{T}
]
\prec
( A^\R{T} V^{-1} A + B^\R{T} P^{-1} B )^{-1}

@]@</small>
This completes the proof of this lemma.

<br>
<br>
<b><big><a name="Simulation" id="Simulation">Simulation</a></big></b>
<br>
Suppose we simulate date
<small>@(@
y \sim \B{N}( A \bar{\theta}, V)
@)@</small> and independent prior values
<small>@(@
z \sim \B{N}( B \bar{\theta}, P)
@)@</small> were
<small>@(@
A
@)@</small>, <small>@(@
V
@)@</small> are as in <a href="posterior.htm#Lemma 1" target="_top"><span style='white-space: nowrap'>Lemma&nbsp;1</span></a>
 and
<small>@(@
B
@)@</small>, <small>@(@
P
@)@</small> are as in <a href="posterior.htm#Lemma 2" target="_top"><span style='white-space: nowrap'>Lemma&nbsp;2</span></a>
.
Furthermore, we define <small>@(@
\hat{\theta}
@)@</small> as the maximizer of
<small>@[@

   \B{p}( y, z | \theta ) = \B{p} ( y | \theta ) \B{p} ( z | \theta )

@]@</small>
We define <small>@(@
w \in \B{R}^{(m + n) \times 1}
@)@</small>,
<small>@(@
C \in \B{R}^{ (m + n) \times n}
@)@</small>, and
<small>@(@
U \in \B{R}^{ (m + n) \times (m + n)}
@)@</small> by
<small>@[@

w = \left( \begin{array}{c} y \\ z \end{array} \right)
\W{,}
C = \left( \begin{array}{cc} A & 0 \\ 0 & B \end{array} \right)
\W{,}
U = \left( \begin{array}{cc} V & 0 \\ 0 & P \end{array} \right)

@]@</small>
We can now apply Lemma 1 with <small>@(@
y
@)@</small> replaced by <small>@(@
w
@)@</small>,
<small>@(@
A
@)@</small> replaced by <small>@(@
C
@)@</small> and
<small>@(@
V
@)@</small> replaced by <small>@(@
U
@)@</small>.
It follows from Lemma 1 that <small>@(@
\B{E} [ \hat{\theta} ] = \bar{\theta}
@)@</small> and
<small>@[@

\B{E} [ ( \hat{\theta} - \bar{\theta} ) ( \hat{\theta} - \bar{\theta} )^\R{T} ]
=
( A^\R{T} V^{-1} A + B^\R{T} P^{-1} B )^{-1}

@]@</small>
which is the proper posterior variance for the case in Lemma 2.


<hr>Input File: omh/model/posterior.omh

</body>
</html>
