$Id$
-----------------------------------------------------------------------------
dismod_at: Estimating Disease Rates as Functions of Age and Time
          Copyright (C) 2014-18 University of Washington
             (Bradley M. Bell bradbell@uw.edu)

This program is distributed under the terms of the
	     GNU Affero General Public License version 3.0 or later
see http://www.gnu.org/licenses/agpl.txt
-----------------------------------------------------------------------------
$begin posterior$$
$escape $$
$spell
	dismod
$$

$section Simulating Posterior Distribution for Model Variables$$

$head Purpose$$
The current version of dismod_at
$code sample$$ command with method equal to
$cref/simulate/sample_command/method/simulate/$$
fits simulated random measurements
in order to sample from the posterior distribution for the parameters.
$cref/Lemma 2/posterior/Lemma 2/$$ below shows that,
because prior information is used with maximum likelihood estimation,
this yields less variation than it should for the posterior distribution.


$head Lemma 1$$
Suppose we are given
a matrix $latex A \in \B{R}^{m \times n}$$,
a positive definite matrix, $latex V \in \B{R}^{m \times m}$$ and
a $latex y \in \B{R}^{m \times 1}$$.
Further suppose that there is an
unknown vector $latex \bar{\theta} \in \B{R}^{n \times 1}$$ such that
$latex y \sim \B{N} ( A \bar{\theta} , V )$$.
The maximum likelihood estimator $latex \hat{\theta}$$
for $latex \bar{\theta}$$ given $latex y$$ has mean
$latex \B{E} [ \hat{\theta} ] = \bar{\theta} $$ and variance
$latex \[
\B{E} [ ( \hat{\theta} - \bar{\theta} ) ( \hat{\theta} - \bar{\theta} )^\R{T} ]
=
( A^\R{T} V^{-1} A )^{-1}
=
- \; \left(
	\frac{ \partial^2 } { \partial \theta^2 } \log \B{p} ( y | \theta )
\right)^{-1}
\] $$


$subhead Proof$$
The probability density for $latex y$$ given $latex \theta$$ is
$latex \[
\B{p} ( y | \theta )
=
\det ( 2 \pi V )^{-1/2} \exp \left[
	- \frac{1}{2} ( y - A \theta )^\R{T} V^{-1} ( y - A \theta  )
\right]
\] $$
Dropping the determinant term,
because it does not depend on $latex \theta$$,
and taking the negative log we get the objective
$latex \[
f ( \theta ) = \frac{1}{2} ( y - A \theta )^\R{T} V^{-1} ( y - A \theta  )
\] $$
and the equivalent problem to minimize $latex f ( \theta )$$ with
respect to $latex \theta \in \B{R}^{n \times 1}$$.
The derivative $latex f^{(1)} ( \theta ) \in \B{R}^{1 \times n}$$
is given by
$latex \[
	f^{(1)} ( \theta ) = - ( y - A \theta )^\R{T} V^{-1} A
\] $$
It follows that
$latex \[
- \frac{ \partial^2 } { \partial \theta^2 } \log \B{p} ( y | \theta )
=
f^{(2)} ( \theta )
=
A^\R{T} V^{-1} A
\] $$
This completes the proof of the equation for the second partial
of $latex \B{p} ( y | \theta )$$ in the statement of the lemma.
$pre
	$$
The maximum likelihood estimate $latex \hat{\theta}$$ satisfies the
equation $latex f^{(1)} ( \hat{\theta} ) = 0$$; i.e.,
$latex \[
\begin{array}{rcl}
0 & = & - ( y - A \hat{\theta} )^\R{T} V^{-1} A
\\
A^\R{T} V^{-1} y  & = & A^\R{T} V^{-1} A \hat{\theta}
\\
\hat{\theta} & =  & ( A^\R{T} V^{-1} A )^{-1} A^\R{T} V^{-1} y
\end{array}
\] $$
Defining $latex e = y - A \bar{\theta}$$, we have
$latex \B{E} [ e ] = 0$$ and
$latex \[
\begin{array}{rcl}
\hat{\theta} & =  &
( A^\R{T} V^{-1} A )^{-1} A^\R{T} V^{-1} ( A \bar{\theta} + e )
\\
\hat{\theta}
& =  &
\bar{\theta} + ( A^\R{T} V^{-1} A )^{-1} A^\R{T} V^{-1} e
\end{array}
\] $$
This expresses the estimate $latex \hat{\theta}$$ as a deterministic
function of the noise $latex e$$.
It follows from the last equation for $latex \hat{\theta}$$ above,
and the fact that $latex \B{E} [ e ] = 0$$,
that $latex \B{E} [ \hat{\theta} ] = \bar{\theta} $$.
This completes the proof of the equation for the expected value
of $latex \hat{\theta}$$ in the statement of the lemma.
$pre
	$$
It also follows, from the equation for $latex \hat{\theta}$$ above, that
$latex \[
\begin{array}{rcl}
( \hat{\theta} - \bar{\theta} ) ( \hat{\theta} - \bar{\theta} )^\R{T}
& = &
( A^\R{T} V^{-1} A )^{-1} A^\R{T}
V^{-1} e e^\R{T} V^{-1}
A ( A^\R{T} V^{-1} A )^{-1}
\\
\B{E} [ ( \hat{\theta} - \bar{\theta} ) ( \hat{\theta} - \bar{\theta} )^\R{T} ]
& = &
( A^\R{T} V^{-1} A )^{-1}
\end{array}
\] $$
This completes the proof of the equation for the covariance of
$latex \hat{\theta} - \bar{\theta} $$ in the statement of the lemma.

$head Remark 1$$
For the case in
$cref/Lemma 1/posterior/Lemma 1/$$, the second partial of
$latex \log \B{p} ( y | \theta )$$ with respect to $latex \theta$$
does not depend on $latex \theta$$ and $latex A V^{-1} A$$ is the
information matrix.

$head Lemma 2$$
Suppose that in addition to all the information in
$cref/Lemma 1/posterior/Lemma 1/$$ we have
a matrix $latex B \in \B{R}^{p \times n}$$,
a positive definite matrix, $latex P \in \B{R}^{p \times p}$$,
and $latex z \in \B{R}^{p \times 1}$$ where we have
independent prior information
$latex B \theta \sim \B{N}( z , P )$$.
Further suppose $latex B$$ has rank $latex n$$.
For this case we define $latex \hat{\theta}$$ as the maximizer of
$latex \B{p}( y | \theta ) \B{p}( \theta )$$.
It follows that
$latex \[
\B{E} [
	( \hat{\theta} - \B{E}[ \hat{\theta} ] )
	( \hat{\theta} - \B{E}[ \hat{\theta} ] )^\R{T}
\prec
( A^\R{T} V^{-1} A + B^\R{T} P^{-1} B )^{-1}
=
- \; \left(
	\frac{ \partial^2 } { \partial \theta^2 }
	\log [ \B{p} ( y | \theta ) \B{p} ( \theta ) ]
\right)^{-1}
\]$$
where $latex \prec$$ means less than in a positive definite matrix sense.

$subhead Proof$$
$latex \[
\B{p} ( y | \theta ) \B{p} ( \theta )
=
\det ( 2 \pi V )^{-1/2} \det ( 2 \pi P )^{-1/2} \exp \left[
- \frac{1}{2}
	( y - A \theta )^\R{T} V^{-1} ( y - A \theta  )
- \frac{1}{2}
	( B \theta - z )^\R{T} P^{-1} ( B \theta - z )
\right]
\] $$
The derivative of the corresponding negative log likelihood is
$latex \[
- ( y - A \theta )^\R{T} V^{-1} A  + ( B \theta - z )^\R{T} P^{-1} B
\] $$
From this point,
the proof of the equation for the second partial is very similar to
Lemma 1 and left to the reader.
$pre
	$$
Setting the derivative to zero, we get the corresponding maximum likelihood
estimate $latex \hat{\theta}$$ satisfies
$latex \[
\begin{array}{rcl}
( y - A \hat{\theta} )^\R{T} V^{-1} A
& = &
( B \hat{\theta} - z )^\R{T} P^{-1} B
\\
y^\R{T} V^{-1} A + z^\R{T} P^{-1} B
& = &
\hat{\theta}^\R{T} A^\R{T} V^{-1} A + \hat{\theta}^\R{T} B^\R{T} P^{-1} B
\\
\hat{\theta}
& = &
( A^\R{T} V^{-1} A + B^\R{T} P^{-1} B )^{-1}
( A^\R{T} V^{-1} y + B^\R{T} P^{-1} z )
\\
\hat{\theta}
& = &
( A^\R{T} V^{-1} A + B^\R{T} P^{-1} B )^{-1}
( A^\R{T} V^{-1} A \bar{\theta} + B^\R{T} P^{-1} z )
+
( A^\R{T} V^{-1} A + B^\R{T} P^{-1} B )^{-1} A^\R{T} V^{-1} e
\end{array}
\] $$
The first term is deterministic and the second term is mean zero.
It follows that
$latex \[
\begin{array}{rcl}
\B{E} [ \hat{\theta} ]
& = &
( A^\R{T} V^{-1} A + B^\R{T} P^{-1} B )^{-1}
( A^\R{T} V^{-1} A \bar{\theta} + B^\R{T} P^{-1} z )
\\
\B{E} [
	( \hat{\theta} - \B{E} [ \hat{\theta} ] )
	( \hat{\theta} - \B{E} [ \hat{\theta} ] )^\R{T}
]
& = &
( A^\R{T} V^{-1} A + B^\R{T} P^{-1} B )^{-1}
A^\R{T} V^{-1} A
( A^\R{T} V^{-1} A + B^\R{T} P^{-1} B )^{-1}
\end{array}
\] $$
Since the matrix $latex B^\R{T} P^{-1} B$$ is positive definite, we have
$latex \[
A^\R{T} V^{-1} A  \prec A^\R{T} V^{-1} A + B^\R{T} P^{-1} B
\] $$
Replacing
$latex A^\R{T} V^{-1} A $$ by
$latex  A^\R{T} V^{-1} A + B^\R{T} P^{-1} B$$
in the center of the previous expression
for the variance of $latex \hat{\theta}$$ we obtain
$latex \[
\B{E} [
	( \hat{\theta} - \B{E} [ \hat{\theta} ] )
	( \hat{\theta} - \B{E} [ \hat{\theta} ] )^\R{T}
]
\prec
( A^\R{T} V^{-1} A + B^\R{T} P^{-1} B )^{-1}
\] $$
This completes the proof of this lemma.

$head Simulation$$
Suppose we simulate date
$latex y \sim \B{N}( A \bar{\theta}, V)$$ and independent prior values
$latex z \sim \B{N}( B \bar{\theta}, P)$$ were
$latex A$$, $latex V$$ are as in $cref/Lemma 1/posterior/Lemma 1/$$ and
$latex B$$, $latex P$$ are as in $cref/Lemma 2/posterior/Lemma 2/$$.
Furthermore, we define $latex \hat{\theta}$$ as the maximizer of
$latex \[
	\B{p}( y, z | \theta ) = \B{p} ( y | \theta ) \B{p} ( z | \theta )
\] $$
We define $latex w \in \B{R}^{(m + n) \times 1}$$,
$latex C \in \B{R}^{ (m + n) \times n}$$, and
$latex U \in \B{R}^{ (m + n) \times (m + n)}$$ by
$latex \[
w = \left( \begin{array}{c} y \\ z \end{array} \right)
\W{,}
C = \left( \begin{array}{cc} A & 0 \\ 0 & B \end{array} \right)
\W{,}
U = \left( \begin{array}{cc} V & 0 \\ 0 & P \end{array} \right)
\] $$
We can now apply Lemma 1 with $latex y$$ replaced by $latex w$$,
$latex A$$ replaced by $latex C$$ and
$latex V$$ replaced by $latex U$$.
It follows from Lemma 1 that $latex \B{E} [ \hat{\theta} ] = \bar{\theta}$$ and
$latex \[
\B{E} [ ( \hat{\theta} - \bar{\theta} ) ( \hat{\theta} - \bar{\theta} )^\R{T} ]
=
( A^\R{T} V^{-1} A + B^\R{T} P^{-1} B )^{-1}
\] $$
which is the proper posterior variance for the case in Lemma 2.

$end
