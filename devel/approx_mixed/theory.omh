$Id:$
-----------------------------------------------------------------------------
dismod_at: Estimating Disease Rates as Functions of Age and Time
          Copyright (C) 2014-15 University of Washington
             (Bradley M. Bell bradbell@uw.edu)

This program is distributed under the terms of the
	     GNU Affero General Public License version 3.0 or later
see http://www.gnu.org/licenses/agpl.txt
-----------------------------------------------------------------------------
$begin approx_mixed_theory$$
$latex \newcommand{\dtheta}[1]{ \frac{\R{d}}{\R{d} \theta_{ #1}} }$$


$section Laplace Approximation for Mixed Effects Models$$
$spell
	Kasper Kristensen
	Anders Nielsen
	Casper Berg
	Hans Skaug
	Bradley Bell
$$

$head Reference$$
TMB: Automatic Differentiation and Laplace Approximation,
Kasper Kristensen, Anders Nielsen, Casper W. Berg, Hans Skaug, Bradley M. Bell,
Journal of Statistical Software, Accepted 2015-02.

$head Joint Negative Log-Likelihood, f(theta, u)$$
The reference above defines $latex f( \theta, u)$$
to be the negative log-likelihood of
$latex y$$, $latex u$$ and $latex \theta$$; i.e.,
$latex \[
	- \log [ \B{p} ( y |  \theta, u ) \B{p} ( u | \theta ) \B{p} ( \theta ) ]
\] $$
We use a different notation and define
$latex \[
	f( \theta, u ) = - \log [ \B{p} ( y |  \theta, u ) \B{p} ( u | \theta ) ]
\] $$

$subhead Smoothness Assumption$$
The function $latex f(\theta, u)$$ is assumed to be smooth.
Note that $cref/joint_like/approx_mixed_joint_like/$$ allows for
non-smooth terms in the form of Laplace densities.

$head Prior Negative Log-Likelihood, g(theta)$$
We use the notation
$latex \[
	g( \theta ) = - \log [ \B{p} ( \theta ) ]
\]$$
The function $latex g(x)$$ may be non-smooth because it
can have absolute values in it (corresponding to the Laplace likelihoods).

$head Objective$$

$subhead h(theta, u)$$
Using this notation, the part of the Laplace approximation that
depends on both the fixed and random effects $latex u$$ is
$latex \[
h( \theta, u )
=
+ \frac{1}{2} \log \det f_{uu}^{(2)} ( \theta, u )
+ f( \theta, u )
- \frac{n}{2} \log ( 2 \pi )
\] $$

$subhead u^(theta)$$
Given the fixed effects,
the corresponding random effects that maximize the joint likelihood
are denoted by
$latex \[
	\hat{u} ( \theta ) = \R{argmin} \; f( \theta, u ) \; \R{w.r.t.} \; u
\] $$

$subhead G(theta)$$
We refer to
$latex \[
	G( \theta ) = h[ \theta , \hat{u} ( \theta ) ]
\] $$
as the prior part of the objective.

$subhead L(theta)$$
The Laplace approximation objective, as a function of the fixed effects, is
$latex \[
L ( \theta )
=
G( \theta ) + g( \theta )
\] $$

$head Joint Part of Objective$$

$subhead Implicit Differentiation$$
Because $latex f(\theta, u)$$ is smooth, we obtain
$latex \[
	f_u^{(1)} [ \theta , \hat{u} ( \theta ) ] = 0
\] $$
From this equation it follows that
$latex \[
\hat{u}^{(1)} ( \theta )
=
- f_{uu}^{(2)} \left[ \theta , \hat{u} ( \theta ) \right]^{-1}
	f_{u \theta}^{(2)} \left[ \theta , \hat{u} ( \theta )  \right]
\]$$
We define the function
$latex \[
F( \theta , u )
=
- f_{uu}^{(2)} ( \theta , u)^{-1} f_{u \theta}^{(2)} ( \theta , u )
\] $$

$subhead U(beta, theta, u)$$
We define  the function
$latex \[
U ( \beta , \theta , u )
=
u - f_{uu}^{(2)} ( \theta , u )^{-1} f_u^{(1)} ( \beta , u  )
\] $$
It follows that
$latex \[
	U \left[ \theta , \theta , \hat{u} ( \theta ) \right] = \hat{u} ( \theta )
\]$$
and
$latex \[
U_{\beta}^{(1)} \left[ \theta, \theta , \hat{u} ( \theta ) \right]
=
- f_{uu}^{(2)} \left[ \theta , \hat{u} ( \theta ) \right]^{-1}
	f_{u \theta}^{(2)} \left[ \theta , \hat{u} ( \theta )  \right]
=
\hat{u}^{(1)} ( \theta )
\] $$

$subhead W(beta, theta, u)$$
We define  the function
$latex \[
W ( \beta , \theta , u )
=
U( \beta , \theta , u )
-
f_{uu}^{(2)} [ \beta , U( \beta , \theta , u) ]^{-1}
	f_u^{(1)} [ \beta , U( \beta , \theta , u)  ]
\] $$
It follows that
$latex \[
	W \left[ \theta , \theta , \hat{u} ( \theta ) \right] = \hat{u} ( \theta )
\] $$
and
$latex \[
W_{\beta} [ \theta , \theta , \hat{u} ( \theta ) ]
=
-
f_{uu}^{(2)} \left[ \theta , \hat{u} ( \theta ) \right]^{-1}
	f_{u \theta}^{(2)} [ \beta , \hat{u} ( \theta )  ]
=
\hat{u}^{(1)} ( \theta )
\] $$
I think that the second partials of $latex W^i$$ w.r.t
$latex \beta$$ are equal to $latex u_i^{(2)} ( \theta )$$,
but I have not yet proven so.

$subhead H(beta, theta, u)$$
Given these facts we define
$latex \[
H( \beta , \theta , u)
=
+ \frac{1}{2} \log \det f_{uu}^{(2)} [ \beta, W( \beta , \theta , u) ]
+ f[ \beta, W( \beta , \theta , u) ]
- \frac{n}{2} \log ( 2 \pi )
\] $$
It follows that
$latex \[
\frac{d}{d \theta } \left( h [ \theta , \hat{u} ( \theta ) ]  \right)
=
H_{\beta}^{(1)} \left[ \theta , \theta , \hat{u} ( \theta ) \right]
\] $$
Note that, for first partials to agree, $latex W$$ could be replaced
by $latex U$$ and
$latex f[ \beta , U( \beta , \theta , u) ]$$.
Also note that for function values; neither $latex U$$ or $latex W$$
need to be computed.

$subhead Derivatives of G$$
This formula enables us to use AD to compute the derivatives of
$latex G( \theta ) = h[ \theta , \hat{u} ( \theta ) ]$$
without having to differentiate the iterative process that computes
$latex \hat{u} ( \theta )$$.
Also note that the iterative process may not have a fixed operation sequence
and might need to be re-taped for each value of $latex \theta$$.

$end
