$Id:$
-----------------------------------------------------------------------------
dismod_at: Estimating Disease Rates as Functions of Age and Time
          Copyright (C) 2014-15 University of Washington
             (Bradley M. Bell bradbell@uw.edu)

This program is distributed under the terms of the
	     GNU Affero General Public License version 3.0 or later
see http://www.gnu.org/licenses/agpl.txt
-----------------------------------------------------------------------------
$begin mixed_cppad_theory$$
$latex \newcommand{\dtheta}[1]{ \frac{\R{d}}{\R{d} \theta_{ #1}} }$$


$section Laplace Approximation for Mixed Effects Models$$
$spell
	cppad
	Kasper Kristensen
	Anders Nielsen
	Casper Berg
	Hans Skaug
	Bradley Bell
$$

$head Reference$$
TMB: Automatic Differentiation and Laplace Approximation,
Kasper Kristensen, Anders Nielsen, Casper W. Berg, Hans Skaug, Bradley M. Bell,
Journal of Statistical Software, Accepted 2015-02.

$head Total Log-Likelihood$$
The reference above defines $latex f( \theta, u)$$
to be the negative log-likelihood of the
$latex z$$, $latex y$$, $latex u$$ and $latex \theta$$; i.e.,
$latex \[
- \log [  \;
	\B{p} ( y |  \theta, u ) \B{p} ( u | \theta )  \;
	\B{p} ( z | \theta )\B{p} ( \theta ) \;
]
\] $$


$head Random Likelihood, f(theta, u)$$
We use $latex f( \theta , u )$$ for the part of the likelihood
that depends on the random effects $latex u$$;
$latex \[
	f( \theta, u ) = - \log [ \B{p} ( y |  \theta, u ) \B{p} ( u | \theta ) ]
\] $$

$subhead Assumption$$
The function $latex f(\theta, u)$$ is assumed to be smooth.
Furthermore, there are no constraints on the value of $latex u$$.

$head Fixed Negative Log-Likelihood, g(theta)$$
We use $latex g( \theta )$$ for the part of the likelihood
that only depends on the fixed effects $latex \theta$$;
$latex \[
	g( \theta ) = - \log [ \B{p} ( z | \theta ) \B{p} ( \theta ) ]
\]$$
The function $latex g( \theta )$$ may not be smooth, to be specific, it
can have absolute values in it (corresponding to the Laplace densities).
Furthermore, there may be  constraints on the value of $latex \theta$$.

$head Objective$$

$subhead h(theta, u)$$
Using the notation above,
negative log of the part of the Laplace approximation that
depends on both the fixed and random effects $latex u$$ is
$latex \[
h( \theta, u )
=
+ \frac{1}{2} \log \det f_{uu}^{(2)} ( \theta, u )
+ f( \theta, u )
- \frac{n}{2} \log ( 2 \pi )
\] $$
where $latex n$$ is the number of random effects.

$subhead u^(theta)$$
Given the fixed effects,
the corresponding random effects that maximize the random likelihood
are denoted by
$latex \[
	\hat{u} ( \theta ) = \R{argmin} \; f( \theta, u ) \; \R{w.r.t.} \; u
\] $$

$subhead Random Part of Objective, r(theta)$$
We refer to
$latex \[
	r( \theta )
	=
	h[ \theta , \hat{u} ( \theta ) ]
	\approx
	- \log \left[ \int_{-\infty}^{+\infty}
		\B{p} ( y |  \theta, u ) \B{p} ( u | \theta ) \; \B{d} u
	\right]
\] $$
as the random part of the objective.

$subhead Total Objective, L(theta)$$
The total objective, as a function of the fixed effects, is
$latex \[
L ( \theta )
=
r( \theta ) + g( \theta )
\] $$

$head Differentiating Random Part of Objective$$

$subhead Implicit Differentiation$$
Because $latex f(\theta, u)$$ is smooth, we obtain
$latex \[
	f_u^{(1)} [ \theta , \hat{u} ( \theta ) ] = 0
\] $$
From this equation it follows that
$latex \[
\hat{u}^{(1)} ( \theta )
=
- f_{uu}^{(2)} \left[ \theta , \hat{u} ( \theta ) \right]^{-1}
	f_{u \theta}^{(2)} \left[ \theta , \hat{u} ( \theta )  \right]
\]$$

$subhead U(beta, theta, u)$$
We define  the function
$latex \[
U ( \beta , \theta , u )
=
u - f_{uu}^{(2)} ( \theta , u )^{-1} f_u^{(1)} ( \beta , u  )
\] $$
It follows that
$latex \[
	U \left[ \theta , \theta , \hat{u} ( \theta ) \right] = \hat{u} ( \theta )
\]$$
and
$latex \[
U_{\beta}^{(1)} \left[ \theta, \theta , \hat{u} ( \theta ) \right]
=
- f_{uu}^{(2)} \left[ \theta , \hat{u} ( \theta ) \right]^{-1}
	f_{u \theta}^{(2)} \left[ \theta , \hat{u} ( \theta )  \right]
=
\hat{u}^{(1)} ( \theta )
\] $$

$subhead W(beta, theta, u)$$
We define  the function
$latex \[
W ( \beta , \theta , u )
=
U( \beta , \theta , u )
-
f_{uu}^{(2)} [ \beta , U( \beta , \theta , u) ]^{-1}
	f_u^{(1)} [ \beta , U( \beta , \theta , u)  ]
\] $$
It follows that
$latex \[
	W \left[ \theta , \theta , \hat{u} ( \theta ) \right] = \hat{u} ( \theta )
\] $$
and
$latex \[
W_{\beta} [ \theta , \theta , \hat{u} ( \theta ) ]
=
-
f_{uu}^{(2)} \left[ \theta , \hat{u} ( \theta ) \right]^{-1}
	f_{u \theta}^{(2)} [ \beta , \hat{u} ( \theta )  ]
=
\hat{u}^{(1)} ( \theta )
\] $$
I think that the second partials of $latex W^i$$ w.r.t
$latex \beta$$ are equal to $latex u_i^{(2)} ( \theta )$$,
but I have not yet proven so.

$subhead Approximate Random Objective, H(beta, theta, u)$$
Given these facts we define
$latex \[
H( \beta , \theta , u)
=
+ \frac{1}{2} \log \det f_{uu}^{(2)} [ \beta, W( \beta , \theta , u) ]
+ f[ \beta, W( \beta , \theta , u) ]
- \frac{n}{2} \log ( 2 \pi )
\] $$
It follow that
$latex \[
r( \theta ) = H( \theta , \theta , \hat{u} ( \theta ) )
\] $$
$latex \[
r^{(1)} ( \theta )
=
H_{\beta}^{(1)} \left[ \theta , \theta , \hat{u} ( \theta ) \right]
\] $$
$latex \[
r^{(2)} ( \theta )
=
H_{\beta \beta}^{(1)} \left[ \theta , \theta , \hat{u} ( \theta ) \right]
\] $$
(The last equation has yet to be proved.)
This enables us to use AD to compute the derivatives of
$latex r( \theta ) = h[ \theta , \hat{u} ( \theta ) ]$$
without having to differentiate the iterative process that computes
$latex \hat{u} ( \theta )$$.
Also note that the iterative process may not have a fixed operation sequence
and might need to be re-taped for each value of $latex \theta$$.
$pre

$$
Note that, for first derivatives to agree, $latex W$$ could be replaced
by $latex U$$ in the definition of $latex H$$ (less computation).
Furthermore, for function values to agree $latex W$$ could be
replaced by $latex \hat{u} ( \theta )$$ (even less computation).

$end
